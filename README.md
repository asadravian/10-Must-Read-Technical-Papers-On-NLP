<h2> 10 Must Read Technical Papers On NLP</h2>

<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/10-Must-Read-Technical-Papers-On-NLP/blob/master/mrt(1).pdf" style="text-decoration:none;">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/10-Must-Read-Technical-Papers-On-NLP/blob/master/mrt(2).pdf" style="text-decoration:none;">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/10-Must-Read-Technical-Papers-On-NLP/blob/master/mrt(3).pdf" style="text-decoration:none;">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/10-Must-Read-Technical-Papers-On-NLP/blob/master/mrt(4).pdf" style="text-decoration:none;">Language Models are Unsupervised Multitask Learners</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/10-Must-Read-Technical-Papers-On-NLP/blob/master/mrt(5).pdf" style="text-decoration:none;">Neural Approaches to Conversational AI: Question Answering, Task-Oriented Dialogues and Social Chatbot</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/10-Must-Read-Technical-Papers-On-NLP/blob/master/mrt(6).pdf" style="text-decoration:none;">Improving Language Understanding by Generative Pre-Training</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/10-Must-Read-Technical-Papers-On-NLP/blob/master/mrt(7).pdf" style="text-decoration:none;">Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/10-Must-Read-Technical-Papers-On-NLP/blob/master/mrt(8).pdf" style="text-decoration:none;"> A Neural Conversational Model </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/10-Must-Read-Technical-Papers-On-NLP/blob/master/mrt(9).pdf" style="text-decoration:none;">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/10-Must-Read-Technical-Papers-On-NLP/blob/master/mrt(10).pdf" style="text-decoration:none;">Bridging the Gap between Training and Inference for Neural Machine Translation </a></li>                              

 </ul>
